{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Assessment 1 - Image Classification [100 marks]\n",
    "\n",
    "<div class=\"logos\"><img src=\"https://drive.google.com/uc?id=132BXgkV5w1bpXlVpdr5BtZdpagqYvna7\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this assessment, you will gain practical experience in:\n",
    "\n",
    "> 1. Implementing and evaluating a multi-layer perceptron (MLP) and convolutional neural network (CNN) in solving a classification problem\n",
    "> 2. Building, evaluating, and finetuning a CNN on an image dataset from development to testing \n",
    "> 3. Tackling overfitting using strategies such as data augmentation and drop out\n",
    "> 4. Fine tuning a model \n",
    "> 5. Comparing the performance of a new model with an off-the-shelf model (AlexNet)\n",
    "> 6. Gaining a deeper understanding of model performance using visualisations from Grad-CAM.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU. We highly recommend you use platforms such as Colab.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from a direct link or the Kaggle challenge website:\n",
    "\n",
    ">[Direct access to data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[Access data through Kaggle webpage](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07) \n",
    "\n",
    "\n",
    "### Required submissions\n",
    "\n",
    "##### 1. Kaggle Competition\n",
    "To participate in the submission of test results, you will need an account. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. [Link to submit your results on Kaggle competition](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/submissions). \n",
    "\n",
    "Please submit only your predictions from test set - detailed instructions are provided in (3)\n",
    "\n",
    "##### 2. Submission of your work\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected images from section 6 \"Failure/success analysis\" (outputs from gradcam, for example you can put these images into failure and succcess folders).\n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sc21xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhiping Xiang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is a package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is a package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is a library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed - examples of importing libraries are provided below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "True\n",
      "True\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)\n",
    "import torch\n",
    "import math\n",
    "torch.device('mps')\n",
    "print(torch.backends.mps.is_available())\n",
    "# True\n",
    "print(torch.backends.mps.is_built())\n",
    "# True\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview:**\n",
    "\n",
    "**1. Function implementation** (12 marks)\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (4 marks)\n",
    "\n",
    "**2. Model training** (20 marks)\n",
    "*   **2.1** Train on TinyImageNet30 dataset (7 marks)\n",
    "*   **2.2** Generate confusion matrices and ROC curves (4 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (9 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "            \n",
    "\n",
    "**3. Model Fine-tuning on CIFAR10 dataset** (20 marks)\n",
    "*   **3.1** Fine-tune your model (initialise your model with pretrained weights from (2)) (8 marks)\n",
    "*   **3.2** Fine-tune model with frozen base convolution layers (8 marks)\n",
    "*   **3.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe? (4 marks) \n",
    "\n",
    "**4. Model testing** (18 marks)\n",
    "*   **4.1**   Test your final model in (2) on test set - code to do this (10 marks)\n",
    "*   **4.2**   Upload your result to Kaggle  (8 marks)\n",
    "\n",
    "**5. Model comparison** (14 marks)\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (6 marks)\n",
    "*   **5.2**   Compare the results of your CNN model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks)\n",
    "\n",
    "**6. Interpretation of results** (16 marks)\n",
    "*   **6.1** Use grad-CAM on your model and on AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare the results from your model and from AlexNet (4 marks)\n",
    "*   **6.3** Comment on (6 marks):\n",
    "    - why the network predictions were correct or not correct in your predictions? \n",
    "    - what can you do to improve your results further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Function implementations [12 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset class [4 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define your own class LoadFromFolder\n",
    "class TinyImageNet30Dataset(Dataset):\n",
    "    def __init__(self, root_dir,classes,transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = classes\n",
    "        self.transform=transform\n",
    "\n",
    "        # Iterate over all classes\n",
    "        for i, c in enumerate(classes):\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            # print(i,c)\n",
    "            if os.path.isdir(class_dir):\n",
    "                # Add all images in this class directory to the dataset\n",
    "                for image_name in os.listdir(class_dir):\n",
    "                    image_path = os.path.join(class_dir, image_name)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(i)\n",
    "        print(len(self.image_paths))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the previously computed number of images\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            # Apply transform\n",
    "            img_tensor = self.transform(img)\n",
    "            # Retrieve the label for this image\n",
    "            label = self.labels[index]\n",
    "            # Convert label to tensor\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# load the dataset\n",
    "train_set_root='./comp5623m-artificial-intelligence/train_set/train_set/train_set'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    " # load the classes\n",
    "file = open(\"./comp5623m-artificial-intelligence/class.txt\", \"r\")\n",
    "contents=[]\n",
    "for line in file.readlines():\n",
    "    curLine=line.strip().split(\"\\t\")\n",
    "    contents.append(curLine[:])\n",
    "classes=[items[1] for items in contents]\n",
    "# print('classes:',classes)\n",
    "\n",
    "batch_size=64\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a MLP model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a MLP Model class\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the input, hidden, and output sizes\n",
    "input_size = 3*64*64 #(channel* height * width)\n",
    "hidden_size =128\n",
    "output_size = 30 # 30 classes\n",
    "\n",
    "# Create the MLP model\n",
    "model_MLP = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function_MLP = nn.CrossEntropyLoss()\n",
    "optimizer_MLP = optim.SGD(model_MLP.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a CNN model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers \n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512)\n",
    "        )\n",
    "        self.reLu1=nn.ReLU()\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.reLu2=nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "def built_model_cnn(CNN_model,learning_rate):\n",
    "    num_classes=30\n",
    "    # Set up the model, loss function, and optimizer\n",
    "    model_CNN = CNN_model(num_classes=num_classes).to(device)\n",
    "    loss_function_CNN = nn.CrossEntropyLoss()\n",
    "    optimizer_CNN = optim.SGD(model_CNN.parameters(), lr=learning_rate,momentum=0.9)\n",
    "    return model_CNN,loss_function_CNN,optimizer_CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training [20 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN models - show loss and accuracy graphs side by side [7 marks]\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. Top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader completed: torch.Size([3, 64, 64])\n",
      "val_dataloader completed: torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_train_val_dataset():\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    test_ratio=0.2\n",
    "\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_size = int(test_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(train_dataloader))\n",
    "    print('train_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(val_dataloader))\n",
    "    print('val_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "    return train_dataloader,val_dataloader\n",
    "\n",
    "train_dataloader,val_dataloader=split_train_val_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top-*k* accuracy \n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[33], line 16\u001B[0m\n\u001B[1;32m     14\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     15\u001B[0m model_MLP\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     17\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mview(images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;66;03m# Flatten input images\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[0;32mIn[27], line 37\u001B[0m, in \u001B[0;36mTinyImageNet30Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     34\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[index]\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mopen(image_path) \u001B[38;5;28;01mas\u001B[39;00m img:\n\u001B[0;32m---> 37\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# Apply transform\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     img_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:949\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    947\u001B[0m         mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGBA\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mor\u001B[39;00m (mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m matrix):\n\u001B[0;32m--> 949\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m matrix:\n\u001B[1;32m    952\u001B[0m     \u001B[38;5;66;03m# matrix conversion\u001B[39;00m\n\u001B[1;32m    953\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:1206\u001B[0m, in \u001B[0;36mImage.copy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1198\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1199\u001B[0m \u001B[38;5;124;03mCopies this image. Use this method if you wish to paste things\u001B[39;00m\n\u001B[1;32m   1200\u001B[0m \u001B[38;5;124;03minto an image, but still retain the original.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m   1204\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload()\n\u001B[0;32m-> 1206\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#TO COMPLETE --> Running you MLP model class\n",
    "epochs = 50\n",
    "\n",
    "# Lists to keep track of training/validation loss and accuracy over epochs\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "best_val_acc = 0.0\n",
    " # Train model\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model_MLP.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "        labels=labels.to(device)\n",
    "        optimizer_MLP.zero_grad()\n",
    "        outputs = model_MLP(images)\n",
    "        loss = loss_function_MLP(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_MLP.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "        train_acc += acc.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model_MLP.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            outputs = model_MLP(images)\n",
    "            loss = loss_function_MLP(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            val_acc += acc.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "    # Save the model if it has the best validation accuracy so far\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_MLP.state_dict(), 'best_model_mlp.pth')\n",
    "\n",
    "# Print loss and accuracy values for current epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph\n",
    "def plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history):\n",
    "    # Plot loss and accuracy values over epochs\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_acc_history, label='Training Accuracy')\n",
    "    plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running you CNN model class\n",
    "\n",
    "def train_val_cnn(model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader):\n",
    "    epochs = 50\n",
    "    # Lists to keep track of training/validation loss and accuracy over epochs\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "     # Train model\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        model_CNN.train()\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            optimizer_CNN.zero_grad()\n",
    "            outputs = model_CNN(images)\n",
    "            loss = loss_function_CNN(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_CNN.step()\n",
    "            train_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            train_acc += acc.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        model_CNN.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device)# Flatten input images\n",
    "                labels=labels.to(device)\n",
    "                outputs = model_CNN(images)\n",
    "                loss = loss_function_CNN(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "                val_acc += acc.item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "     # Save the model if it has the best validation accuracy so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            if  best_val_acc>50 :\n",
    "                torch.save(model_CNN.state_dict(), 'best_model_cnn.pth')\n",
    "                if val_loss<1 and val_acc>99:\n",
    "                    break\n",
    "\n",
    "    # Print loss and accuracy values for current epoch\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    return train_loss_history,val_loss_history,train_acc_history,val_acc_history\n",
    "\n",
    "learning_rate=0.001\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "train_loss_history,val_loss_history,train_acc_history,val_acc_history=train_val_cnn(model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on your model and results that should include number of parameters in each model and why CNN over MLP for image classification task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves [4 marks]\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way that clearly indicates what percentage of the data is represented in each position.\n",
    "- Display ROC curve for 5 top classes with area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Load the model\n",
    "\n",
    "# Load the trained model\n",
    "# Load the saved model\n",
    "model=model_CNN\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Create lists to store predictions and labels\n",
    "train_preds = []\n",
    "train_labels = []\n",
    "val_preds = []\n",
    "val_labels = []\n",
    "# Create empty lists for true labels and predicted probabilities:\n",
    "val_labels_roc = []\n",
    "val_probs = []\n",
    "\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "for inputs, labels in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        train_preds.extend(preds.tolist())\n",
    "        train_labels.extend(labels.tolist())\n",
    "\n",
    "print('get predictions for the training set successfully!')\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        val_preds.extend(preds.tolist())\n",
    "        val_labels.extend(labels.tolist())\n",
    "        # append to roc\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        # print(probs.size()) # check the size of probs\n",
    "        val_labels_roc.append(labels.cpu().numpy())\n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "\n",
    "print('get predictions and probabilities for the validation set successfully!')\n",
    "\n",
    "# Compute the confusion matrices\n",
    "train_cm = confusion_matrix(train_labels, train_preds)\n",
    "val_cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Normalize the confusion matrices\n",
    "train_cm_norm = train_cm.astype('float') / train_cm.sum(axis=1)[:, np.newaxis]\n",
    "val_cm_norm = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Print the confusion matrices\n",
    "print(\"Training Set Confusion Matrix:\")\n",
    "print(train_cm_norm)\n",
    "print(\"Validation Set Confusion Matrix:\")\n",
    "print(val_cm_norm)\n",
    "\n",
    "\n",
    "# ROC\n",
    "# Concatenate the true labels and predicted probabilities into numpy arrays\n",
    "print(len(val_probs))\n",
    "val_labels_roc = np.concatenate(val_labels_roc)\n",
    "val_probs = np.concatenate(val_probs)\n",
    "\n",
    "# Compute the ROC curves and AUCs for each class using scikit-learn:\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(30):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(val_labels) == i, val_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# sort top 5\n",
    "top_classes = sorted(range(30), key=lambda i: roc_auc[i], reverse=True)[:5]\n",
    "# plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in top_classes:\n",
    "    plt.plot(fpr[i], tpr[i], label='Class {}, AUC = {:.2f}'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves for top 5 classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: All parts below here relate to the CNN model only and not the MLP! You are advised to use your final CNN model only for each of the following parts.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strategies for tackling overfitting (9 marks)\n",
    "Using your (final) CNN model, use the strategies below to avoid overfitting. You can reuse the network weights from previous training, often referred to as ``fine tuning``. \n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data augmentation\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations. \n",
    "\n",
    "> Provide graph and comment on what you observe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# import  transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.2)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n",
    "train_dataloader,val_dataloader=split_train_val_dataset()\n",
    "\n",
    "# train the model\n",
    "learning_rate=0.02\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "print('build model successfully!')\n",
    "train_loss_history,val_loss_history,train_acc_history,val_acc_history=train_val_cnn(model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader)\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dropout\n",
    "\n",
    "> Implement dropout in your model \n",
    "\n",
    "> Provide graph and comment on your choice of proportion used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "class CNNWithDropout(CNN):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNNWithDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        x = self.dropout(x)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,learning_rate)\n",
    "\n",
    "train_loss_history,val_loss_history,train_acc_history,val_acc_history=train_val_cnn(model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader)\n",
    "\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Hyperparameter tuning\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001]\n",
    "\n",
    "> Provide separate graphs for loss and accuracy, each showing performance at three different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "learning_rates=[0.1,0.001,0.0001]\n",
    "train_loss_history_lr=[]\n",
    "val_loss_history_lr=[]\n",
    "train_acc_history_lr=[]\n",
    "val_acc_history_lr=[]\n",
    "for lr in learning_rates:\n",
    "    model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,lr)\n",
    "    train_loss_history,val_loss_history,train_acc_history,val_acc_history=train_val_cnn(model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader)\n",
    "    train_loss_history_lr.append(train_loss_history)\n",
    "    val_loss_history_lr.append(val_loss_history)\n",
    "    train_acc_history_lr.append(train_acc_history)\n",
    "    val_acc_history_lr.append(val_acc_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph\n",
    "# Plot loss and accuracy values over epochs\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_loss_history_lr[i], label='Training Loss, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_loss_history_lr[i], label='Validation Loss, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_acc_history_lr[i], label='Training Accuracy, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_acc_history_lr[i], label='Validation Accuracy, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model testing [18 marks]\n",
    "Online evaluation of your model performance on the test set.\n",
    "\n",
    "> Prepare the dataloader for test set\n",
    "\n",
    "> Write evaluation code for writing predictions\n",
    "\n",
    "> Upload it to Kaggle submission page [link](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Test class and predictions [10 marks]\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions \n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here!\n",
    "\n",
    "# Load the test dataset\n",
    "test_set_root='./comp5623m-artificial-intelligence/test_set/test_set/'\n",
    "test_dataset = datasets.ImageFolder(test_set_root, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Create a list of image filenames for the test set\n",
    "image_filenames = [os.path.basename(path) for path, _ in test_dataset.imgs]\n",
    "\n",
    "# do predicitions\n",
    "\n",
    "# Load the saved model\n",
    "model=model_CNN\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "# Make predictions on the test dataset\n",
    "all_predictions = []\n",
    "for inputs, _ in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs=inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print('predictions of all test images successfully! ')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle [8 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: Id and Category (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "Please note you will get marks for higher performance.\n",
    "\n",
    "The Id column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image in the test set and 1 row for the headers. [To submit please visit](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [8 marks]. The class leaderboard will not affect marking (brownie points!).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import csv\n",
    " # Save the image names and predicted classes to a CSV file\n",
    "with open('sc21xz.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Category'])\n",
    "    for i in range(len(test_dataset)):\n",
    "        writer.writerow([image_filenames[i], all_predictions[i]])\n",
    "print('CSV file successfully saved!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [20 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one task and then tunes or tweaks the model to make it perform a second similar task. You can perform finetuning in the following way:\n",
    "- Train an entire model from scratch (large dataset, more computation)\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation) \n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10``, [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here are only 10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Apply transfer learning initialise with pretrained model weights\n",
    "Use pretrained weights from AlexNet only (on the right of figure) to initialise your model. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg\" style=\"width:1000px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: Two models are given here: LeNet and AlexNet for image classification. However, you have to use **only AlexNet**.</center></caption>\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compare above configurations and comment on comparative performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Model comparisons\n",
    "We often need to compare our model with other state-of-the-art methods to understand how well it performs compared to existing architectures. Here you will thus compare your model design with AlexNet on the TinyImageNet30 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Finetune AlexNet on TinyImageNet30\n",
    "> Load AlexNet as you did above\n",
    "\n",
    "> Train AlexNet on TinyImageNet30 dataset until convergence. Make sure you use the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Compare results on validation set of TinyImageNet30\n",
    "> Loss graph, top1 accuracy, confusion matrix and execution time for your model (say, mymodel and AlexNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Interpretation of results (16 marks)\n",
    "\n",
    "> Please use TinyImageNet30 dataset for all results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 6.1-6.2 Implement grad-CAM and visualise results (10 marks)\n",
    "\n",
    "- Use an existing library to initiate grad-CAM \n",
    "\n",
    "        - To install: !pip install torchcam\n",
    "        - Call SmoothGradCAMpp: from torchcam.methods import SmoothGradCAMpp\n",
    "        - Apply to your model \n",
    "\n",
    "You can see the details here: https://github.com/frgfm/torch-cam\n",
    "\n",
    "- Apply grad-CAM to your model on at least four correctly classified images\n",
    "- Apply grad-CAM on retrained AlexNet on at least four incorrectly classified images\n",
    "\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "\n",
    "**HINT for displaying images with grad-CAM:**\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap superimposed onto the original image. We recommend the following steps to get a clear meaningful display. \n",
    "\n",
    "From torchcam.utils import overlay_mask. But remember to resize your image, normalise it and put a 1 for the batch dimension (e.g, [1, 3, 224, 224]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 Your comments on (6 marks):\n",
    "> a) Why model predictions were correct or incorrect? You can support your case from 6.2\n",
    "\n",
    "> b) What can you do to improve your results further?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---> Double click to respond here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission. Use our teams channel to seek any help!**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
