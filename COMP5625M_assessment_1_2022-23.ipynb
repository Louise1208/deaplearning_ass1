{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## COMP5625M Assessment 1 - Image Classification [100 marks]\n",
    "\n",
    "<div class=\"logos\"><img src=\"https://drive.google.com/uc?id=132BXgkV5w1bpXlVpdr5BtZdpagqYvna7\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Through this assessment, you will gain practical experience in:\n",
    "\n",
    "> 1. Implementing and evaluating a multi-layer perceptron (MLP) and convolutional neural network (CNN) in solving a classification problem\n",
    "> 2. Building, evaluating, and finetuning a CNN on an image dataset from development to testing\n",
    "> 3. Tackling overfitting using strategies such as data augmentation and drop out\n",
    "> 4. Fine tuning a model\n",
    "> 5. Comparing the performance of a new model with an off-the-shelf model (AlexNet)\n",
    "> 6. Gaining a deeper understanding of model performance using visualisations from Grad-CAM.\n",
    "\n",
    "\n",
    "### Setup and resources\n",
    "\n",
    "You must work using this template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU. We highly recommend you use platforms such as Colab.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from a direct link or the Kaggle challenge website:\n",
    "\n",
    ">[Direct access to data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[Access data through Kaggle webpage](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "\n",
    "### Required submissions\n",
    "\n",
    "##### 1. Kaggle Competition\n",
    "To participate in the submission of test results, you will need an account. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. [Link to submit your results on Kaggle competition](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/submissions).\n",
    "\n",
    "Please submit only your predictions from test set - detailed instructions are provided in (3)\n",
    "\n",
    "##### 2. Submission of your work\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected images from section 6 \"Failure/success analysis\" (outputs from gradcam, for example you can put these images into failure and succcess folders).\n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sc21xz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your full name:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zhiping Xiang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is a package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is a package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is a library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed - examples of importing libraries are provided below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)\n",
    "import torch\n",
    "import math\n",
    "torch.device('mps')\n",
    "# True\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# print(torch.cuda.is_available())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30.\n",
    "\n",
    "### **Overview:**\n",
    "\n",
    "**1. Function implementation** (12 marks)\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (4 marks)\n",
    "\n",
    "**2. Model training** (20 marks)\n",
    "*   **2.1** Train on TinyImageNet30 dataset (7 marks)\n",
    "*   **2.2** Generate confusion matrices and ROC curves (4 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (9 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "\n",
    "**3. Model Fine-tuning on CIFAR10 dataset** (20 marks)\n",
    "*   **3.1** Fine-tune your model (initialise your model with pretrained weights from (2)) (8 marks)\n",
    "*   **3.2** Fine-tune model with frozen base convolution layers (8 marks)\n",
    "*   **3.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe? (4 marks)\n",
    "\n",
    "**4. Model testing** (18 marks)\n",
    "*   **4.1**   Test your final model in (2) on test set - code to do this (10 marks)\n",
    "*   **4.2**   Upload your result to Kaggle  (8 marks)\n",
    "\n",
    "**5. Model comparison** (14 marks)\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (6 marks)\n",
    "*   **5.2**   Compare the results of your CNN model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks)\n",
    "\n",
    "**6. Interpretation of results** (16 marks)\n",
    "*   **6.1** Use grad-CAM on your model and on AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare the results from your model and from AlexNet (4 marks)\n",
    "*   **6.3** Comment on (6 marks):\n",
    "    - why the network predictions were correct or not correct in your predictions?\n",
    "    - what can you do to improve your results further?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Function implementations [12 marks]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Dataset class [4 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define your own class LoadFromFolder\n",
    "class TinyImageNet30Dataset(Dataset):\n",
    "    def __init__(self, root_dir,classes,transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = classes\n",
    "        self.transform=transform\n",
    "\n",
    "        # Iterate over all classes\n",
    "        for i, c in enumerate(classes):\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            # print(i,c)\n",
    "            if os.path.isdir(class_dir):\n",
    "                # Add all images in this class directory to the dataset\n",
    "                for image_name in os.listdir(class_dir):\n",
    "                    image_path = os.path.join(class_dir, image_name)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(i)\n",
    "        print(len(self.image_paths))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the previously computed number of images\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            # Apply transform\n",
    "            img_tensor = self.transform(img)\n",
    "            # Retrieve the label for this image\n",
    "            label = self.labels[index]\n",
    "            # Convert label to tensor\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# load the dataset\n",
    "train_set_root='./comp5623m-artificial-intelligence/train_set/train_set/train_set'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    " # load the classes\n",
    "file = open(\"./comp5623m-artificial-intelligence/class.txt\", \"r\")\n",
    "contents=[]\n",
    "for line in file.readlines():\n",
    "    curLine=line.strip().split(\"\\t\")\n",
    "    contents.append(curLine[:])\n",
    "classes=[items[1] for items in contents]\n",
    "# print('classes:',classes)\n",
    "\n",
    "batch_size=64\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define a MLP model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a MLP Model class\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the input, hidden, and output sizes\n",
    "input_size = 3*64*64 #(channel* height * width)\n",
    "hidden_size =128\n",
    "output_size = 30 # 30 classes\n",
    "\n",
    "# Create the MLP model\n",
    "model_MLP = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function_MLP = nn.CrossEntropyLoss()\n",
    "optimizer_MLP = optim.SGD(model_MLP.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Define a CNN model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers\n",
    "- Loss function\n",
    "- Optimiser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512)\n",
    "        )\n",
    "        self.reLu1=nn.ReLU()\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.reLu2=nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "def built_model_cnn(CNN_model,learning_rate):\n",
    "    num_classes=30\n",
    "    # Set up the model, loss function, and optimizer\n",
    "    model_CNN = CNN_model(num_classes=num_classes).to(device)\n",
    "    loss_function_CNN = nn.CrossEntropyLoss()\n",
    "    optimizer_CNN = optim.SGD(model_CNN.parameters(), lr=learning_rate,momentum=0.9)\n",
    "    return model_CNN,loss_function_CNN,optimizer_CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Model training [20 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN models - show loss and accuracy graphs side by side [7 marks]\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. Top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader completed: torch.Size([3, 64, 64])\n",
      "val_dataloader completed: torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_train_val_dataset(dataset):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    test_ratio=0.2\n",
    "\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_size = int(test_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(train_dataloader))\n",
    "    print('train_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(val_dataloader))\n",
    "    print('val_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "    return train_dataloader,val_dataloader\n",
    "\n",
    "train_dataloader,val_dataloader=split_train_val_dataset(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define top-*k* accuracy\n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 3.3368, Train Acc: 8.60%, Val Loss: 3.2582, Val Acc: 11.17%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m labels\u001B[38;5;241m=\u001B[39mlabels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     19\u001B[0m optimizer_MLP\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_MLP\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function_MLP(outputs, labels)\n\u001B[1;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[5], line 18\u001B[0m, in \u001B[0;36mMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu1(x)\n\u001B[1;32m     17\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n\u001B[0;32m---> 18\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc3(x)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/activation.py:102\u001B[0m, in \u001B[0;36mReLU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py:1457\u001B[0m, in \u001B[0;36mrelu\u001B[0;34m(input, inplace)\u001B[0m\n\u001B[1;32m   1455\u001B[0m     result \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrelu_(\u001B[38;5;28minput\u001B[39m)\n\u001B[1;32m   1456\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1457\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1458\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#TO COMPLETE --> Running you MLP model class\n",
    "epochs =30\n",
    "\n",
    "# Lists to keep track of training/validation loss and accuracy over epochs\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "best_val_acc = 0.0\n",
    " # Train model\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model_MLP.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "        labels=labels.to(device)\n",
    "        optimizer_MLP.zero_grad()\n",
    "        outputs = model_MLP(images)\n",
    "        loss = loss_function_MLP(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_MLP.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "        train_acc += acc.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model_MLP.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            outputs = model_MLP(images)\n",
    "            loss = loss_function_MLP(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            val_acc += acc.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "    # Save the model if it has the best validation accuracy so far\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_MLP.state_dict(), 'best_model_mlp.pth')\n",
    "\n",
    "# Print loss and accuracy values for current epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "def plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history):\n",
    "    # Plot loss and accuracy values over epochs\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_acc_history, label='Training Accuracy')\n",
    "    plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 68\u001B[0m\n\u001B[1;32m     66\u001B[0m learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m\n\u001B[1;32m     67\u001B[0m model_CNN,loss_function_CNN,optimizer_CNN\u001B[38;5;241m=\u001B[39mbuilt_model_cnn(CNN,learning_rate)\n\u001B[0;32m---> 68\u001B[0m train_loss_history_mymodel2,val_loss_history_mymodel2,train_acc_history_mymodel2,val_acc_history_mymodel2,best_val_acc_mymodel2\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_val_cnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_function_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbest_val_acc\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 18\u001B[0m, in \u001B[0;36mtrain_val_cnn\u001B[0;34m(epochs, model_CNN, loss_function_CNN, optimizer_CNN, train_dataloader, val_dataloader, best_val_acc)\u001B[0m\n\u001B[1;32m     16\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     17\u001B[0m model_CNN\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     19\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;66;03m# Flatten input images\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 37\u001B[0m, in \u001B[0;36mTinyImageNet30Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     34\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[index]\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mopen(image_path) \u001B[38;5;28;01mas\u001B[39;00m img:\n\u001B[0;32m---> 37\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# Apply transform\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     img_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:937\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m    891\u001B[0m ):\n\u001B[1;32m    892\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 937\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    939\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    941\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[0;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#TO COMPLETE --> Running you CNN model class\n",
    "best_val_acc = 0.0\n",
    "def train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader,best_val_acc):\n",
    "\n",
    "    # Lists to keep track of training/validation loss and accuracy over epochs\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_acc =best_val_acc\n",
    "\n",
    "     # Train model\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        model_CNN.train()\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            optimizer_CNN.zero_grad()\n",
    "            outputs = model_CNN(images)\n",
    "            loss = loss_function_CNN(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_CNN.step()\n",
    "            train_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            train_acc += acc.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        model_CNN.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device)# Flatten input images\n",
    "                labels=labels.to(device)\n",
    "                outputs = model_CNN(images)\n",
    "                loss = loss_function_CNN(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "                val_acc += acc.item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "     # Save the model if it has the best validation accuracy so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_CNN.state_dict(), 'best_model_cnn.pth')\n",
    "        if train_acc>99:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Print loss and accuracy values for current epoch\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    return train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc\n",
    "\n",
    "epochs = 50\n",
    "learning_rate=0.01\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "train_loss_history_mymodel2,val_loss_history_mymodel2,train_acc_history_mymodel2,val_acc_history_mymodel2,best_val_acc_mymodel2=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader,best_val_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "plot_loss_acc(train_loss_history_mymodel2,val_loss_history_mymodel2,train_acc_history_mymodel2,val_acc_history_mymodel2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Comment on your model and results that should include number of parameters in each model and why CNN over MLP for image classification task?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves [4 marks]\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way that clearly indicates what percentage of the data is represented in each position.\n",
    "- Display ROC curve for 5 top classes with area under the curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CNN'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 28\u001B[0m\n\u001B[1;32m     24\u001B[0m val_probs \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Evaluate the model on the training set\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m inputs, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m     30\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 37\u001B[0m, in \u001B[0;36mTinyImageNet30Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     34\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimage_paths[index]\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Image\u001B[38;5;241m.\u001B[39mopen(image_path) \u001B[38;5;28;01mas\u001B[39;00m img:\n\u001B[0;32m---> 37\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     38\u001B[0m     \u001B[38;5;66;03m# Apply transform\u001B[39;00m\n\u001B[1;32m     39\u001B[0m     img_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform(img)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:937\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    889\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\n\u001B[1;32m    890\u001B[0m     \u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mPalette\u001B[38;5;241m.\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m\n\u001B[1;32m    891\u001B[0m ):\n\u001B[1;32m    892\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    893\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    894\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    934\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    935\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 937\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    939\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    940\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    941\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/ImageFile.py:269\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    266\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(msg)\n\u001B[1;32m    268\u001B[0m b \u001B[38;5;241m=\u001B[39m b \u001B[38;5;241m+\u001B[39m s\n\u001B[0;32m--> 269\u001B[0m n, err_code \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Load the model\n",
    "\n",
    "# Load the trained model\n",
    "# Load the saved model\n",
    "model=model_CNN\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Create lists to store predictions and labels\n",
    "train_preds = []\n",
    "train_labels = []\n",
    "\n",
    "# Create empty lists for true labels and predicted probabilities:\n",
    "val_labels_roc = []\n",
    "val_probs = []\n",
    "\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "for inputs, labels in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        train_preds.extend(preds.tolist())\n",
    "        train_labels.extend(labels.tolist())\n",
    "\n",
    "print('get predictions for the training set successfully!')\n",
    "preds_alex = []\n",
    "val_labels = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_alex.extend(preds.tolist())\n",
    "        val_labels.extend(labels.tolist())\n",
    "        # append to roc\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        # print(probs.size()) # check the size of probs\n",
    "        val_labels_roc.append(labels.cpu().numpy())\n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "\n",
    "print('get predictions and probabilities for the validation set successfully!')\n",
    "\n",
    "# Compute the confusion matrices\n",
    "train_cm = confusion_matrix(train_labels, train_preds)\n",
    "val_cm = confusion_matrix(val_labels, preds_alex)\n",
    "\n",
    "# Normalize the confusion matrices\n",
    "train_cm_norm = train_cm.astype('float') / train_cm.sum(axis=1)[:, np.newaxis]\n",
    "val_cm_norm = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Print the confusion matrices\n",
    "print(\"Training Set Confusion Matrix:\")\n",
    "print(train_cm_norm)\n",
    "print(\"Validation Set Confusion Matrix:\")\n",
    "print(val_cm_norm)\n",
    "\n",
    "\n",
    "# ROC\n",
    "# Concatenate the true labels and predicted probabilities into numpy arrays\n",
    "print(len(val_probs))\n",
    "val_labels_roc = np.concatenate(val_labels_roc)\n",
    "val_probs = np.concatenate(val_probs)\n",
    "\n",
    "# Compute the ROC curves and AUCs for each class using scikit-learn:\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(30):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(val_labels) == i, val_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# sort top 5\n",
    "top_classes = sorted(range(30), key=lambda i: roc_auc[i], reverse=True)[:5]\n",
    "# plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in top_classes:\n",
    "    plt.plot(fpr[i], tpr[i], label='Class {}, AUC = {:.2f}'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves for top 5 classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: All parts below here relate to the CNN model only and not the MLP! You are advised to use your final CNN model only for each of the following parts.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Strategies for tackling overfitting (9 marks)\n",
    "Using your (final) CNN model, use the strategies below to avoid overfitting. You can reuse the network weights from previous training, often referred to as ``fine tuning``.\n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1 Data augmentation\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations.\n",
    "\n",
    "> Provide graph and comment on what you observe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500\n",
      "torch.Size([3, 64, 64])\n",
      "train_dataloader completed: torch.Size([3, 64, 64])\n",
      "val_dataloader completed: torch.Size([3, 64, 64])\n",
      "build model successfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 27\u001B[0m\n\u001B[1;32m     25\u001B[0m model_CNN,loss_function_CNN,optimizer_CNN\u001B[38;5;241m=\u001B[39mbuilt_model_cnn(CNN,learning_rate)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbuild model successfully!\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 27\u001B[0m train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_val_cnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_function_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_dataloader_try\u001B[49m\u001B[43m,\u001B[49m\u001B[43mval_dataloader_try\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbest_val_acc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)\n",
      "Cell \u001B[0;32mIn[11], line 18\u001B[0m, in \u001B[0;36mtrain_val_cnn\u001B[0;34m(epochs, model_CNN, loss_function_CNN, optimizer_CNN, train_dataloader, val_dataloader, best_val_acc)\u001B[0m\n\u001B[1;32m     16\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     17\u001B[0m model_CNN\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     19\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;66;03m# Flatten input images\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 39\u001B[0m, in \u001B[0;36mTinyImageNet30Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     37\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Apply transform\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m img_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Retrieve the label for this image\u001B[39;00m\n\u001B[1;32m     41\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/transforms.py:1255\u001B[0m, in \u001B[0;36mColorJitter.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m   1253\u001B[0m         img \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39madjust_saturation(img, saturation_factor)\n\u001B[1;32m   1254\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m fn_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m hue_factor \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1255\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_hue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhue_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/functional.py:949\u001B[0m, in \u001B[0;36madjust_hue\u001B[0;34m(img, hue_factor)\u001B[0m\n\u001B[1;32m    947\u001B[0m     _log_api_usage_once(adjust_hue)\n\u001B[1;32m    948\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 949\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madjust_hue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhue_factor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    951\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39madjust_hue(img, hue_factor)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/functional_pil.py:119\u001B[0m, in \u001B[0;36madjust_hue\u001B[0;34m(img, hue_factor)\u001B[0m\n\u001B[1;32m    116\u001B[0m     np_h \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39muint8(hue_factor \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m255\u001B[39m)\n\u001B[1;32m    117\u001B[0m h \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(np_h, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 119\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmerge\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHSV\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_mode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:1081\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m   1078\u001B[0m     dither \u001B[38;5;241m=\u001B[39m Dither\u001B[38;5;241m.\u001B[39mFLOYDSTEINBERG\n\u001B[1;32m   1080\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1081\u001B[0m     im \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdither\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[1;32m   1083\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1084\u001B[0m         \u001B[38;5;66;03m# normalize source image and try again\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "# import  transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform_try = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.RandomVerticalFlip(p=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.ColorJitter(brightness=0.1,  saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform_try)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image\n",
    "\n",
    "train_dataloader_try,val_dataloader_try=split_train_val_dataset(dataset)\n",
    "\n",
    "learning_rate=0.01\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "print('build model successfully!')\n",
    "train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2 Dropout\n",
    "\n",
    "> Implement dropout in your model\n",
    "\n",
    "> Provide graph and comment on your choice of proportion used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 26\u001B[0m\n\u001B[1;32m     22\u001B[0m epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m\n\u001B[1;32m     24\u001B[0m model_CNN,loss_function_CNN,optimizer_CNN\u001B[38;5;241m=\u001B[39mbuilt_model_cnn(CNNWithDropout,learning_rate)\n\u001B[0;32m---> 26\u001B[0m train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel,best_val_acc\u001B[38;5;241m=\u001B[39m\u001B[43mtrain_val_cnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43mmodel_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_function_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43moptimizer_CNN\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtrain_dataloader_try\u001B[49m\u001B[43m,\u001B[49m\u001B[43mval_dataloader_try\u001B[49m\u001B[43m,\u001B[49m\u001B[43mbest_val_acc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m plot_loss_acc(train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel)\n",
      "Cell \u001B[0;32mIn[11], line 18\u001B[0m, in \u001B[0;36mtrain_val_cnn\u001B[0;34m(epochs, model_CNN, loss_function_CNN, optimizer_CNN, train_dataloader, val_dataloader, best_val_acc)\u001B[0m\n\u001B[1;32m     16\u001B[0m train_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     17\u001B[0m model_CNN\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     19\u001B[0m     images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;66;03m# Flatten input images\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     labels\u001B[38;5;241m=\u001B[39mlabels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    627\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 628\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    631\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    632\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    669\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    670\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 671\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    673\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     56\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/utils/data/dataset.py:295\u001B[0m, in \u001B[0;36mSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(idx, \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m    294\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m idx]]\n\u001B[0;32m--> 295\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 39\u001B[0m, in \u001B[0;36mTinyImageNet30Dataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     37\u001B[0m img \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Apply transform\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m img_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;66;03m# Retrieve the label for this image\u001B[39;00m\n\u001B[1;32m     41\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[index]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/transforms.py:135\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torchvision/transforms/functional.py:163\u001B[0m, in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;66;03m# handle PIL Image\u001B[39;00m\n\u001B[1;32m    162\u001B[0m mode_to_nptype \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mint32, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI;16\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mint16, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m: np\u001B[38;5;241m.\u001B[39mfloat32}\n\u001B[0;32m--> 163\u001B[0m img \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode_to_nptype\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muint8\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m pic\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    166\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255\u001B[39m \u001B[38;5;241m*\u001B[39m img\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:691\u001B[0m, in \u001B[0;36mImage.__array_interface__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    687\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    688\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__array_interface__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    689\u001B[0m     \u001B[38;5;66;03m# numpy array interface support\u001B[39;00m\n\u001B[1;32m    690\u001B[0m     new \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m--> 691\u001B[0m     shape, typestr \u001B[38;5;241m=\u001B[39m \u001B[43m_conv_type_shape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    692\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshape\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m shape\n\u001B[1;32m    693\u001B[0m     new[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtypestr\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m typestr\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/PIL/Image.py:254\u001B[0m, in \u001B[0;36m_conv_type_shape\u001B[0;34m(im)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extra \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    253\u001B[0m     shape \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (extra,)\n\u001B[0;32m--> 254\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m shape, \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtypestr\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "class CNNWithDropout(CNN):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNNWithDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        x = self.dropout(x)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "epochs=50\n",
    "# train the model\n",
    "learning_rate=0.005\n",
    "epochs=50\n",
    "\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,learning_rate)\n",
    "\n",
    "train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "\n",
    "plot_loss_acc(train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.3 Hyperparameter tuning\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001]\n",
    "\n",
    "> Provide separate graphs for loss and accuracy, each showing performance at three different learning rates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "learning_rates=[0.1, 0.001, 0.0001] #0.1, 0.001, 0.0001\n",
    "train_loss_history_lr=[]\n",
    "val_loss_history_lr=[]\n",
    "train_acc_history_lr=[]\n",
    "val_acc_history_lr=[]\n",
    "epochs=30\n",
    "for lr in learning_rates:\n",
    "    print('learning rate is:',lr)\n",
    "    model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,lr)\n",
    "    train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "    train_loss_history_lr.append(train_loss_history)\n",
    "    val_loss_history_lr.append(val_loss_history)\n",
    "    train_acc_history_lr.append(train_acc_history)\n",
    "    val_acc_history_lr.append(val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "# Plot loss and accuracy values over epochs\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_loss_history_lr[i], label='Training Loss, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_loss_history_lr[i], label='Validation Loss, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_acc_history_lr[i], label='Training Accuracy, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_acc_history_lr[i], label='Validation Accuracy, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 Model testing [18 marks]\n",
    "Online evaluation of your model performance on the test set.\n",
    "\n",
    "> Prepare the dataloader for test set\n",
    "\n",
    "> Write evaluation code for writing predictions\n",
    "\n",
    "> Upload it to Kaggle submission page [link](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3.1 Test class and predictions [10 marks]\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CNNWithDropout'>\n",
      "predictions of all test images successfully! \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here!\n",
    "\n",
    "# Load the test dataset\n",
    "test_set_root='./comp5623m-artificial-intelligence/test_set/test_set/'\n",
    "test_dataset = datasets.ImageFolder(test_set_root, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Create a list of image filenames for the test set\n",
    "image_filenames = [os.path.basename(path) for path, _ in test_dataset.imgs]\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "model=CNNWithDropout(num_classes=30)\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "# Make predictions on the test dataset\n",
    "all_predictions = []\n",
    "for inputs, _ in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs=inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print('predictions of all test images successfully! ')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle [8 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: Id and Category (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "Please note you will get marks for higher performance.\n",
    "\n",
    "The Id column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image in the test set and 1 row for the headers. [To submit please visit](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [8 marks]. The class leaderboard will not affect marking (brownie points!).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file successfully saved!\n"
     ]
    }
   ],
   "source": [
    "# Your code here!\n",
    "import csv\n",
    " # Save the image names and predicted classes to a CSV file\n",
    "with open('sc21xz.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Category'])\n",
    "    for i in range(len(test_dataset)):\n",
    "        writer.writerow([image_filenames[i], all_predictions[i]])\n",
    "print('CSV file successfully saved!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [20 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one task and then tunes or tweaks the model to make it perform a second similar task. You can perform finetuning in the following way:\n",
    "- Train an entire model from scratch (large dataset, more computation)\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation)\n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10``, [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here are only 10*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Define transforms for the training data and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "batch_size=64\n",
    "# Download the dataset\n",
    "dataset_cifr10 = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Split the dataset into training and validation set\n",
    "train_dataloader_CIFAR10, val_dataloader_CIFAR10 = split_train_val_dataset(dataset_cifr10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import torchvision.models as models\n",
    "# Load pretrained AlexNet\n",
    "model_alexnet = models.alexnet(pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 Apply transfer learning initialise with pretrained model weights\n",
    "Use pretrained weights from AlexNet only (on the right of figure) to initialise your model.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg\" style=\"width:1000px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: Two models are given here: LeNet and AlexNet for image classification. However, you have to use **only AlexNet**.</center></caption>\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters\n",
    "import copy\n",
    "# No frozen layers\n",
    "\n",
    "for param in model_alexnet.parameters():\n",
    "    param.requires_grad = True\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "# Copy the model\n",
    "model_alexnet_c1=copy.deepcopy(model_alexnet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters\n",
    "\n",
    "# Frozen base convolution blocks\n",
    "for name, param in model_alexnet.features.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3 Compare above configurations and comment on comparative performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell\n",
    "def train_val_alex(epochs,model,loss_function, optimizer,train_dataloader, validation_dataloader):\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    train_acc_history=[]\n",
    "    val_acc_history=[]\n",
    "    best_val_acc=0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0\n",
    "        train_acc=0\n",
    "        val_loss=0\n",
    "        val_acc=0\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            train_acc += acc.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss/len(train_dataloader))\n",
    "        train_acc_history.append(train_acc/len(train_dataloader))\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(validation_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "                val_acc += acc.item()\n",
    "        val_loss /= len(validation_dataloader)\n",
    "        val_acc /= len(validation_dataloader)\n",
    "        val_loss_history.append(val_loss/len(validation_dataloader))\n",
    "        val_acc_history.append(val_acc/len(validation_dataloader))\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model_alex.pth')\n",
    "    return train_loss_history,val_loss_history,train_acc_history,val_acc_history\n",
    "\n",
    "\n",
    "\n",
    "print('start!')\n",
    "epochs = 10\n",
    "learning_rate=0.001\n",
    "\n",
    "# use model_c1:\n",
    "model_alex_c1 = model_alexnet_c1\n",
    "# output of alexnet is 1000, change it into 10\n",
    "model_alex_c1.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "# Define the loss function and optimizer\n",
    "loss_function_alex = nn.CrossEntropyLoss()\n",
    "optimizer_alex = torch.optim.Adam(model_alex_c1.classifier[6].parameters(), lr=learning_rate)\n",
    "# train the model\n",
    "model_alexnet_c1.to(device)\n",
    "train_loss_history_c1,val_loss_history_c1,train_acc_history_c1,val_acc_history_c1=train_val_alex(epochs, model_alex_c1, loss_function_alex, optimizer_alex,train_dataloader_CIFAR10, val_dataloader_CIFAR10 )\n",
    "\n",
    "# use model_c2:\n",
    "model_alex_c2=model_alexnet\n",
    "# output of alexnet is 1000, change it into 10\n",
    "model_alex_c2.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "# Define the  optimizer\n",
    "optimizer_alex = torch.optim.SGD(model_alex_c2.parameters(), lr=learning_rate)\n",
    "# train the model\n",
    "model_alex_c2.to(device)\n",
    "train_loss_history_c2,val_loss_history_c2,train_acc_history_c2,val_acc_history_c2=train_val_alex(epochs, model_alex_c2, loss_function_alex, optimizer_alex, train_dataloader_CIFAR10, val_dataloader_CIFAR10)\n",
    "\n",
    "\n",
    "# Plot training and validation loss and accuracy\n",
    "# Plot loss and accuracy values over epochs\n",
    "plt.plot(train_loss_history_c1, label='Training Loss_c1')\n",
    "plt.plot(val_loss_history_c1, label='Validation Loss_c1')\n",
    "plt.plot(train_loss_history_c2, label='Training Loss_c2')\n",
    "plt.plot(val_loss_history_c2, label='Validation Loss_c2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_history_c1, label='Training Accuracy_c1')\n",
    "plt.plot(val_acc_history_c1, label='Validation Accuracy_c1')\n",
    "plt.plot(train_acc_history_c2, label='Training Accuracy_c2')\n",
    "plt.plot(val_acc_history_c2, label='Validation Accuracy_c2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 Model comparisons\n",
    "We often need to compare our model with other state-of-the-art methods to understand how well it performs compared to existing architectures. Here you will thus compare your model design with AlexNet on the TinyImageNet30 dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1 Finetune AlexNet on TinyImageNet30\n",
    "> Load AlexNet as you did above\n",
    "\n",
    "> Train AlexNet on TinyImageNet30 dataset until convergence. Make sure you use the same dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# load dataset\n",
    "transform_alex= transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform_alex)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "train_dataloader,val_dataloader=split_train_val_dataset(dataset)\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "# load AlexNet\n",
    "model_alex = model_alexnet\n",
    "# change output into 30\n",
    "model_alex.classifier[-1] = torch.nn.Linear(4096, 30)\n",
    "\n",
    "# Define the parameters for training\n",
    "epochs = 30\n",
    "learning_rate=0.01\n",
    "#\n",
    "# AlexNet\n",
    "epochs = 50\n",
    "learning_rate=0.005\n",
    "loss_function_alex = nn.CrossEntropyLoss()\n",
    "model_alex = model_alex.to(device)\n",
    "optimizer_alex = torch.optim.SGD(model_alex.parameters(), lr=learning_rate,momentum=0.9)\n",
    "train_loss_history_alex,val_loss_history_alex,train_acc_history_alex,val_acc_history_alex=train_val_alex(epochs, model_alex, loss_function_alex, optimizer_alex, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2 Compare results on validation set of TinyImageNet30\n",
    "> Loss graph, top1 accuracy, confusion matrix and execution time for your model (say, mymodel and AlexNet)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import time\n",
    "\n",
    "# loss graph\n",
    "plt.plot(val_loss_history_alex, label='Validation Loss_alex')\n",
    "plt.plot(val_loss_history_mymodel, label='Validation Loss_mymodel')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# top1 accuracy\n",
    "plt.plot(val_acc_history_alex, label='Validation Accuracy_alex')\n",
    "plt.plot(val_acc_history_mymodel, label='Validation Accuracy_mymodel')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# load CNN model\n",
    "model_cnn=model_CNN\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model_cnn))\n",
    "\n",
    "# load model_alex\n",
    "model_alex=model_alexnet\n",
    "model_alex.load_state_dict(torch.load('best_model_alex.pth'))\n",
    "print(type(model_alex))\n",
    "\n",
    "# confusion matrix of CNN\n",
    "model_cnn.eval()\n",
    "start_time_cnn = time.time()\n",
    "# confusion matrix\n",
    "preds_cnn = []\n",
    "labels_cnn = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model_cnn(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_cnn.extend(preds.tolist())\n",
    "        labels_cnn.extend(labels.tolist())\n",
    "print('get predictions for the CNN set successfully!')\n",
    "end_time_cnn = time.time()\n",
    "\n",
    "\n",
    "# confusion matrix of AlexNet\n",
    "start_time_alex = time.time()\n",
    "model_alex.eval()\n",
    "# confusion matrix\n",
    "preds_alex = []\n",
    "labels_alex = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model_alex(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_alex.extend(preds.tolist())\n",
    "        labels_alex.extend(labels.tolist())\n",
    "print('get predictions for the Alexnet set successfully!')\n",
    "end_time_alex = time.time()\n",
    "\n",
    "# Compute the confusion matrices\n",
    "cm_alex = confusion_matrix(labels_alex, preds_alex)\n",
    "# Normalize the confusion matrices\n",
    "cm_norm_alex = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "# Print the confusion matrices\n",
    "print(\"AlexNet Confusion Matrix:\")\n",
    "print(cm_norm_alex)\n",
    "\n",
    "# Compute the confusion matrices\n",
    "cm_cnn = confusion_matrix(labels_cnn, preds_cnn)\n",
    "# Normalize the confusion matrices\n",
    "cm_norm_cnn = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "# Print the confusion matrices\n",
    "print(\"My Model Confusion Matrix:\")\n",
    "print(cm_norm_cnn)\n",
    "\n",
    "\n",
    "# compute execution time\n",
    "execution_time = end_time_alex - start_time_alex\n",
    "print('execution time of AlexNet is: ', execution_time)\n",
    "execution_time = end_time_cnn - start_time_cnn\n",
    "print('execution time of CNN is: ', execution_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6 Interpretation of results (16 marks)\n",
    "\n",
    "> Please use TinyImageNet30 dataset for all results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 6.1-6.2 Implement grad-CAM and visualise results (10 marks)\n",
    "\n",
    "- Use an existing library to initiate grad-CAM\n",
    "\n",
    "        - To install: !pip install torchcam\n",
    "        - Call SmoothGradCAMpp: from torchcam.methods import SmoothGradCAMpp\n",
    "        - Apply to your model\n",
    "\n",
    "You can see the details here: https://github.com/frgfm/torch-cam\n",
    "\n",
    "- Apply grad-CAM to your model on at least four correctly classified images\n",
    "- Apply grad-CAM on retrained AlexNet on at least four incorrectly classified images\n",
    "\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "\n",
    "**HINT for displaying images with grad-CAM:**\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap superimposed onto the original image. We recommend the following steps to get a clear meaningful display.\n",
    "\n",
    "From torchcam.utils import overlay_mask. But remember to resize your image, normalise it and put a 1 for the batch dimension (e.g, [1, 3, 224, 224])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "image_paths=[]\n",
    "labels=[]\n",
    "root_dir='./comp5623m-artificial-intelligence/train_set/train_set/train_set'\n",
    "# load images_cnn_correct\n",
    "# Iterate over all classes\n",
    "for i, c in enumerate(classes):\n",
    "    class_dir = os.path.join(root_dir, c)\n",
    "    # print(i,c)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # Add all images in this class directory to the dataset\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(i)\n",
    "print(len(image_paths))\n",
    "\n",
    "def get_gradcam(model,model_name, image_paths, transform):\n",
    "\n",
    "    pred=0\n",
    "    model.eval()\n",
    "    # Loop over the images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Load the image and preprocess it\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        preprocess = transform\n",
    "        image_tensor= preprocess(image)\n",
    "        input_tensor = image_tensor.unsqueeze(0)\n",
    "        # Get the predicted class index\n",
    "        output = model(input_tensor)\n",
    "        # Retrieve the CAM by passing the class index and the model output\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        if predicted.item()==labels[i] and model_name == 'cnn':\n",
    "            print('predicted class is: ', predicted.item())\n",
    "            print('true class is: ', labels[i])\n",
    "            print('correctly classified')\n",
    "            pred+=1\n",
    "            cam_extractor = SmoothGradCAMpp(model, target_layer='layer3.3')\n",
    "            # Preprocess your data and feed it to the model\n",
    "            out = model(input_tensor)\n",
    "            # Retrieve the CAM by passing the class index and the model output\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "            # Resize the CAM and overlay it\n",
    "            result = overlay_mask(to_pil_image(image_tensor), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "            # Display it\n",
    "            plt.imshow(result);\n",
    "            plt.axis('off');\n",
    "            plt.tight_layout();\n",
    "            plt.show()\n",
    "            # Show the ordinary image using the default viewer on your system\n",
    "            img = plt.imread(image_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        if predicted.item()!=labels[i] and model_name == 'alexnet':\n",
    "            print('predicted class is: ', predicted.item())\n",
    "            print('true class is: ', labels[i])\n",
    "            print('incorrectly classified!')\n",
    "            pred+=1\n",
    "\n",
    "            cam_extractor = SmoothGradCAMpp(model, target_layer='features.12')\n",
    "            # Preprocess your data and feed it to the model\n",
    "            out = model(input_tensor)\n",
    "            # Retrieve the CAM by passing the class index and the model output\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "            # Resize the CAM and overlay it\n",
    "            result = overlay_mask(to_pil_image(image_tensor), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "            # Display it\n",
    "            plt.imshow(result);\n",
    "            plt.axis('off');\n",
    "            plt.tight_layout();\n",
    "            plt.show()\n",
    "            # Show the ordinary image using the default viewer on your system\n",
    "            img = plt.imread(image_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        if pred==4:\n",
    "                break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model\n",
    "learning_rate=0.01\n",
    "model_cnn=CNNWithDropout(num_classes=30)\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model_cnn))\n",
    "# model_cnn.to(device)\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "get_gradcam(model_cnn, 'cnn',image_paths, transform_cnn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#AlexNet\n",
    "# # load model_alex\n",
    "model_alex = models.alexnet(pretrained=True)\n",
    "for param in model_alexnet.parameters():\n",
    "    param.requires_grad = True\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "# Frozen base convolution blocks\n",
    "for name, param in model_alexnet.features.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "model_alex.classifier[6] = torch.nn.Linear(4096, 30)\n",
    "model_alex.load_state_dict(torch.load('best_model_alex.pth'))\n",
    "print(type(model_alex))\n",
    "transform_alex= transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "get_gradcam(model_alex, 'alexnet',image_paths, transform_alex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.3 Your comments on (6 marks):\n",
    "> a) Why model predictions were correct or incorrect? You can support your case from 6.2\n",
    "\n",
    "> b) What can you do to improve your results further?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a)\n",
    "The heat map generated by SmoothGradCAMpp shows which areas of the model play a decisive role in the prediction results during the prediction process.\n",
    "\n",
    "In 6.2, we output the comparison between the heat map and the original map to help analyse the reason why the model prediction is good or not.\n",
    "\n",
    "When using the CNN model to classify images, although the correctly classified heat map generally matches our intuition and corresponds to the features in the original image (e.g. it can be basically judged where the objects that need to be recognized in the picture are distributed, such as the Figure 2 and Figure 3), but the specific characteristics cannot be fully clearly identified (such as the limbs and heads of animals in class 0 are not recognized, such as Figure 3).And for Figure 1 and Figure 4, even the main object positions are not been recognized. Therefore, we found that although the general features were captured, it was not very effective in helping the model make correct predictions. This may be the reason why the CNN model can accurately classify these images but the accuracy rate is still relatively low.\n",
    "\n",
    "\n",
    "When using the AlexNet model to classify images, its incorrectly classified heat map is completely inconsistent with our intuition. Even the distribution area of key objects is not located. In fact, the heat map basically focuses on the four corners of the picture. This may be the reason why the accuracy of the AlexNet model is relatively high, but these pictures are still unable to be accurately classified.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b)\n",
    "To improve results, the best way is to increase the amount of data in the training set. So that the model can better identify key features and increase the importance of these features. For example, for the CNN model, it is necessary to strengthen the identification of some key features. And it is same for the ALexnet model. Increasing the amount of data can help it distinguish key features to improve training accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission. Use our teams channel to seek any help!**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
