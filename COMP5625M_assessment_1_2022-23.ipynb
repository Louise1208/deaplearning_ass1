{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## COMP5625M Assessment 1 - Image Classification [100 marks]\n",
    "\n",
    "<div class=\"logos\"><img src=\"https://drive.google.com/uc?id=132BXgkV5w1bpXlVpdr5BtZdpagqYvna7\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "The maximum number of marks for each part are shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 25% of the final grade for the module.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Through this assessment, you will gain practical experience in:\n",
    "\n",
    "> 1. Implementing and evaluating a multi-layer perceptron (MLP) and convolutional neural network (CNN) in solving a classification problem\n",
    "> 2. Building, evaluating, and finetuning a CNN on an image dataset from development to testing\n",
    "> 3. Tackling overfitting using strategies such as data augmentation and drop out\n",
    "> 4. Fine tuning a model\n",
    "> 5. Comparing the performance of a new model with an off-the-shelf model (AlexNet)\n",
    "> 6. Gaining a deeper understanding of model performance using visualisations from Grad-CAM.\n",
    "\n",
    "\n",
    "### Setup and resources\n",
    "\n",
    "You must work using this template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU. We highly recommend you use platforms such as Colab.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from a direct link or the Kaggle challenge website:\n",
    "\n",
    ">[Direct access to data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[Access data through Kaggle webpage](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "\n",
    "### Required submissions\n",
    "\n",
    "##### 1. Kaggle Competition\n",
    "To participate in the submission of test results, you will need an account. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb``.\n",
    "\n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. [Link to submit your results on Kaggle competition](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/submissions).\n",
    "\n",
    "Please submit only your predictions from test set - detailed instructions are provided in (3)\n",
    "\n",
    "##### 2. Submission of your work\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected images from section 6 \"Failure/success analysis\" (outputs from gradcam, for example you can put these images into failure and succcess folders).\n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sc21xz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your full name:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Zhiping Xiang"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is a package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is a package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is a library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed - examples of importing libraries are provided below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)\n",
    "import torch\n",
    "import math\n",
    "torch.device('mps')\n",
    "# True\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# print(torch.cuda.is_available())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30.\n",
    "\n",
    "### **Overview:**\n",
    "\n",
    "**1. Function implementation** (12 marks)\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (4 marks)\n",
    "\n",
    "**2. Model training** (20 marks)\n",
    "*   **2.1** Train on TinyImageNet30 dataset (7 marks)\n",
    "*   **2.2** Generate confusion matrices and ROC curves (4 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (9 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "\n",
    "**3. Model Fine-tuning on CIFAR10 dataset** (20 marks)\n",
    "*   **3.1** Fine-tune your model (initialise your model with pretrained weights from (2)) (8 marks)\n",
    "*   **3.2** Fine-tune model with frozen base convolution layers (8 marks)\n",
    "*   **3.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe? (4 marks)\n",
    "\n",
    "**4. Model testing** (18 marks)\n",
    "*   **4.1**   Test your final model in (2) on test set - code to do this (10 marks)\n",
    "*   **4.2**   Upload your result to Kaggle  (8 marks)\n",
    "\n",
    "**5. Model comparison** (14 marks)\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (6 marks)\n",
    "*   **5.2**   Compare the results of your CNN model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks)\n",
    "\n",
    "**6. Interpretation of results** (16 marks)\n",
    "*   **6.1** Use grad-CAM on your model and on AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare the results from your model and from AlexNet (4 marks)\n",
    "*   **6.3** Comment on (6 marks):\n",
    "    - why the network predictions were correct or not correct in your predictions?\n",
    "    - what can you do to improve your results further?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 Function implementations [12 marks]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Dataset class [4 marks]\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from natsort import natsorted\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define your own class LoadFromFolder\n",
    "class TinyImageNet30Dataset(Dataset):\n",
    "    def __init__(self, root_dir,classes,transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.classes = classes\n",
    "        self.transform=transform\n",
    "\n",
    "        # Iterate over all classes\n",
    "        for i, c in enumerate(classes):\n",
    "            class_dir = os.path.join(root_dir, c)\n",
    "            # print(i,c)\n",
    "            if os.path.isdir(class_dir):\n",
    "                # Add all images in this class directory to the dataset\n",
    "                for image_name in os.listdir(class_dir):\n",
    "                    image_path = os.path.join(class_dir, image_name)\n",
    "                    self.image_paths.append(image_path)\n",
    "                    self.labels.append(i)\n",
    "        print(len(self.image_paths))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the previously computed number of images\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            # Apply transform\n",
    "            img_tensor = self.transform(img)\n",
    "            # Retrieve the label for this image\n",
    "            label = self.labels[index]\n",
    "            # Convert label to tensor\n",
    "            label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13500\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# load the dataset\n",
    "train_set_root='./comp5623m-artificial-intelligence/train_set/train_set/train_set'\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5,0.5)\n",
    "])\n",
    "\n",
    " # load the classes\n",
    "file = open(\"./comp5623m-artificial-intelligence/class.txt\", \"r\")\n",
    "contents=[]\n",
    "for line in file.readlines():\n",
    "    curLine=line.strip().split(\"\\t\")\n",
    "    contents.append(curLine[:])\n",
    "classes=[items[1] for items in contents]\n",
    "# print('classes:',classes)\n",
    "\n",
    "batch_size=64\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Define a MLP model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# TO COMPLETE\n",
    "# define a MLP Model class\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the input, hidden, and output sizes\n",
    "input_size = 3*64*64 #(channel* height * width)\n",
    "hidden_size =128\n",
    "output_size = 30 # 30 classes\n",
    "\n",
    "# Create the MLP model\n",
    "model_MLP = MLP(input_size, hidden_size, output_size).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_function_MLP = nn.CrossEntropyLoss()\n",
    "optimizer_MLP = optim.SGD(model_MLP.parameters(), lr=0.01)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Define a CNN model class [4 marks]\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers\n",
    "- Loss function\n",
    "- Optimiser"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer4=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512)\n",
    "        )\n",
    "        self.reLu1=nn.ReLU()\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.reLu2=nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "\n",
    "        return out\n",
    "\n",
    "def built_model_cnn(CNN_model,learning_rate):\n",
    "    num_classes=30\n",
    "    # Set up the model, loss function, and optimizer\n",
    "    model_CNN = CNN_model(num_classes=num_classes).to(device)\n",
    "    loss_function_CNN = nn.CrossEntropyLoss()\n",
    "    optimizer_CNN = optim.SGD(model_CNN.parameters(), lr=learning_rate,momentum=0.9)\n",
    "    return model_CNN,loss_function_CNN,optimizer_CNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 Model training [20 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN models - show loss and accuracy graphs side by side [7 marks]\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. Top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader completed: torch.Size([3, 64, 64])\n",
      "val_dataloader completed: torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "def split_train_val_dataset(dataset):\n",
    "    # Set seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # Split the dataset into train and validation sets\n",
    "    test_ratio=0.2\n",
    "\n",
    "\n",
    "    # Split the dataset into training and validation sets\n",
    "    val_size = int(test_ratio * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset,batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(train_dataloader))\n",
    "    print('train_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "\n",
    "    # Get a single batch of data\n",
    "    images, labels = next(iter(val_dataloader))\n",
    "    print('val_dataloader completed:',next(iter(images)).shape)  # prints shape of image with single batch\n",
    "    return train_dataloader,val_dataloader\n",
    "\n",
    "train_dataloader,val_dataloader=split_train_val_dataset(dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Define top-*k* accuracy\n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Train Loss: 3.3321, Train Acc: 8.51%, Val Loss: 3.2600, Val Acc: 10.54%\n",
      "Epoch [2/30], Train Loss: 3.1648, Train Acc: 12.97%, Val Loss: 3.1125, Val Acc: 13.08%\n",
      "Epoch [3/30], Train Loss: 3.0179, Train Acc: 15.64%, Val Loss: 3.0042, Val Acc: 15.30%\n",
      "Epoch [4/30], Train Loss: 2.9094, Train Acc: 18.17%, Val Loss: 2.9315, Val Acc: 16.84%\n",
      "Epoch [5/30], Train Loss: 2.8264, Train Acc: 19.90%, Val Loss: 2.8822, Val Acc: 18.00%\n",
      "Epoch [6/30], Train Loss: 2.7609, Train Acc: 21.89%, Val Loss: 2.8446, Val Acc: 18.76%\n",
      "Epoch [7/30], Train Loss: 2.7064, Train Acc: 23.16%, Val Loss: 2.8182, Val Acc: 18.87%\n",
      "Epoch [8/30], Train Loss: 2.6587, Train Acc: 24.14%, Val Loss: 2.7978, Val Acc: 19.92%\n",
      "Epoch [9/30], Train Loss: 2.6107, Train Acc: 25.52%, Val Loss: 2.7783, Val Acc: 20.07%\n",
      "Epoch [10/30], Train Loss: 2.5669, Train Acc: 27.06%, Val Loss: 2.7758, Val Acc: 21.41%\n",
      "Epoch [11/30], Train Loss: 2.5207, Train Acc: 28.72%, Val Loss: 2.7537, Val Acc: 21.86%\n",
      "Epoch [12/30], Train Loss: 2.4743, Train Acc: 29.68%, Val Loss: 2.7379, Val Acc: 21.61%\n",
      "Epoch [13/30], Train Loss: 2.4277, Train Acc: 31.31%, Val Loss: 2.7365, Val Acc: 21.71%\n",
      "Epoch [14/30], Train Loss: 2.3831, Train Acc: 32.85%, Val Loss: 2.7314, Val Acc: 22.66%\n",
      "Epoch [15/30], Train Loss: 2.3346, Train Acc: 34.06%, Val Loss: 2.7179, Val Acc: 22.23%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model_MLP(images)\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function_MLP(outputs, labels)\n\u001B[0;32m---> 22\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     23\u001B[0m optimizer_MLP\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     24\u001B[0m train_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    487\u001B[0m     )\n\u001B[0;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#TO COMPLETE --> Running you MLP model class\n",
    "epochs =30\n",
    "\n",
    "# Lists to keep track of training/validation loss and accuracy over epochs\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "best_val_acc = 0.0\n",
    " # Train model\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model_MLP.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "        labels=labels.to(device)\n",
    "        optimizer_MLP.zero_grad()\n",
    "        outputs = model_MLP(images)\n",
    "        loss = loss_function_MLP(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_MLP.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "        train_acc += acc.item()\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    model_MLP.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_dataloader:\n",
    "            images = images.view(images.size(0), -1).to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            outputs = model_MLP(images)\n",
    "            loss = loss_function_MLP(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            val_acc += acc.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_acc /= len(val_dataloader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "    # Save the model if it has the best validation accuracy so far\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_MLP.state_dict(), 'best_model_mlp.pth')\n",
    "\n",
    "# Print loss and accuracy values for current epoch\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "def plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history):\n",
    "    # Plot loss and accuracy values over epochs\n",
    "    plt.plot(train_loss_history, label='Training Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_acc_history, label='Training Accuracy')\n",
    "    plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running you CNN model class\n",
    "best_val_acc = 0.0\n",
    "def train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader,best_val_acc):\n",
    "\n",
    "    # Lists to keep track of training/validation loss and accuracy over epochs\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    best_val_acc =best_val_acc\n",
    "\n",
    "     # Train model\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        model_CNN.train()\n",
    "        for images, labels in train_dataloader:\n",
    "            images = images.to(device)# Flatten input images\n",
    "            labels=labels.to(device)\n",
    "            optimizer_CNN.zero_grad()\n",
    "            outputs = model_CNN(images)\n",
    "            loss = loss_function_CNN(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_CNN.step()\n",
    "            train_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            train_acc += acc.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0\n",
    "        val_acc = 0\n",
    "        model_CNN.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images = images.to(device)# Flatten input images\n",
    "                labels=labels.to(device)\n",
    "                outputs = model_CNN(images)\n",
    "                loss = loss_function_CNN(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "                val_acc += acc.item()\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_acc /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "     # Save the model if it has the best validation accuracy so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model_CNN.state_dict(), 'best_model_cnn.pth')\n",
    "        if train_acc>99:\n",
    "                break\n",
    "\n",
    "\n",
    "    # Print loss and accuracy values for current epoch\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    return train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc\n",
    "\n",
    "epochs = 50\n",
    "learning_rate=0.01\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "train_loss_history_mymodel2,val_loss_history_mymodel2,train_acc_history_mymodel2,val_acc_history_mymodel2,best_val_acc_mymodel2=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader,val_dataloader,best_val_acc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "plot_loss_acc(train_loss_history_mymodel2,val_loss_history_mymodel2,train_acc_history_mymodel2,val_acc_history_mymodel2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Comment on your model and results that should include number of parameters in each model and why CNN over MLP for image classification task?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves [4 marks]\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way that clearly indicates what percentage of the data is represented in each position.\n",
    "- Display ROC curve for 5 top classes with area under the curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Load the model\n",
    "\n",
    "# Load the trained model\n",
    "# Load the saved model\n",
    "model=model_CNN\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Create lists to store predictions and labels\n",
    "train_preds = []\n",
    "train_labels = []\n",
    "\n",
    "# Create empty lists for true labels and predicted probabilities:\n",
    "val_labels_roc = []\n",
    "val_probs = []\n",
    "\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "for inputs, labels in train_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        train_preds.extend(preds.tolist())\n",
    "        train_labels.extend(labels.tolist())\n",
    "\n",
    "print('get predictions for the training set successfully!')\n",
    "preds_alex = []\n",
    "val_labels = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_alex.extend(preds.tolist())\n",
    "        val_labels.extend(labels.tolist())\n",
    "        # append to roc\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        # print(probs.size()) # check the size of probs\n",
    "        val_labels_roc.append(labels.cpu().numpy())\n",
    "        val_probs.append(probs.cpu().numpy())\n",
    "\n",
    "print('get predictions and probabilities for the validation set successfully!')\n",
    "\n",
    "# Compute the confusion matrices\n",
    "train_cm = confusion_matrix(train_labels, train_preds)\n",
    "val_cm = confusion_matrix(val_labels, preds_alex)\n",
    "\n",
    "# Normalize the confusion matrices\n",
    "train_cm_norm = train_cm.astype('float') / train_cm.sum(axis=1)[:, np.newaxis]\n",
    "val_cm_norm = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# Print the confusion matrices\n",
    "print(\"Training Set Confusion Matrix:\")\n",
    "print(train_cm_norm)\n",
    "print(\"Validation Set Confusion Matrix:\")\n",
    "print(val_cm_norm)\n",
    "\n",
    "\n",
    "# ROC\n",
    "# Concatenate the true labels and predicted probabilities into numpy arrays\n",
    "print(len(val_probs))\n",
    "val_labels_roc = np.concatenate(val_labels_roc)\n",
    "val_probs = np.concatenate(val_probs)\n",
    "\n",
    "# Compute the ROC curves and AUCs for each class using scikit-learn:\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(30):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(val_labels) == i, val_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# sort top 5\n",
    "top_classes = sorted(range(30), key=lambda i: roc_auc[i], reverse=True)[:5]\n",
    "# plt\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for i in top_classes:\n",
    "    plt.plot(fpr[i], tpr[i], label='Class {}, AUC = {:.2f}'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curves for top 5 classes')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: All parts below here relate to the CNN model only and not the MLP! You are advised to use your final CNN model only for each of the following parts.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Strategies for tackling overfitting (9 marks)\n",
    "Using your (final) CNN model, use the strategies below to avoid overfitting. You can reuse the network weights from previous training, often referred to as ``fine tuning``.\n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.1 Data augmentation\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations.\n",
    "\n",
    "> Provide graph and comment on what you observe\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# import  transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform\n",
    "transform_try = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.1),\n",
    "    transforms.RandomVerticalFlip(p=0.1),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1,0.1), scale=(0.9,1.1)),\n",
    "    transforms.ColorJitter(brightness=0.1,  saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform_try)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(dataloader))\n",
    "print(next(iter(images)).shape)  # prints shape of image\n",
    "\n",
    "train_dataloader_try,val_dataloader_try=split_train_val_dataset(dataset)\n",
    "\n",
    "learning_rate=0.01\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNN,learning_rate)\n",
    "print('build model successfully!')\n",
    "train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "plot_loss_acc(train_loss_history,val_loss_history,train_acc_history,val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.2 Dropout\n",
    "\n",
    "> Implement dropout in your model\n",
    "\n",
    "> Provide graph and comment on your choice of proportion used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "class CNNWithDropout(CNN):\n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNNWithDropout, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        x = self.dropout(x)\n",
    "        out=self.layer4(out)\n",
    "        out=self.reLu1(out)\n",
    "        out = self.fc1(out)\n",
    "        out=self.reLu2(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "epochs=50\n",
    "# train the model\n",
    "learning_rate=0.005\n",
    "epochs=50\n",
    "\n",
    "model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,learning_rate)\n",
    "\n",
    "train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "\n",
    "plot_loss_acc(train_loss_history_mymodel,val_loss_history_mymodel,train_acc_history_mymodel,val_acc_history_mymodel)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3.3 Hyperparameter tuning\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001]\n",
    "\n",
    "> Provide separate graphs for loss and accuracy, each showing performance at three different learning rates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "learning_rates=[0.1, 0.001, 0.0001] #0.1, 0.001, 0.0001\n",
    "train_loss_history_lr=[]\n",
    "val_loss_history_lr=[]\n",
    "train_acc_history_lr=[]\n",
    "val_acc_history_lr=[]\n",
    "epochs=30\n",
    "for lr in learning_rates:\n",
    "    print('learning rate is:',lr)\n",
    "    model_CNN,loss_function_CNN,optimizer_CNN=built_model_cnn(CNNWithDropout,lr)\n",
    "    train_loss_history,val_loss_history,train_acc_history,val_acc_history,best_val_acc=train_val_cnn(epochs,model_CNN,loss_function_CNN,optimizer_CNN,train_dataloader_try,val_dataloader_try,best_val_acc)\n",
    "    train_loss_history_lr.append(train_loss_history)\n",
    "    val_loss_history_lr.append(val_loss_history)\n",
    "    train_acc_history_lr.append(train_acc_history)\n",
    "    val_acc_history_lr.append(val_acc_history)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graph\n",
    "# Plot loss and accuracy values over epochs\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_loss_history_lr[i], label='Training Loss, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_loss_history_lr[i], label='Validation Loss, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for i in range(len(learning_rates)):\n",
    "    plt.plot(train_acc_history_lr[i], label='Training Accuracy, lr='+str(learning_rates[i]))\n",
    "    plt.plot(val_acc_history_lr[i], label='Validation Accuracy, lr='+str(learning_rates[i]))\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3 Model testing [18 marks]\n",
    "Online evaluation of your model performance on the test set.\n",
    "\n",
    "> Prepare the dataloader for test set\n",
    "\n",
    "> Write evaluation code for writing predictions\n",
    "\n",
    "> Upload it to Kaggle submission page [link](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 3.1 Test class and predictions [10 marks]\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Your code here!\n",
    "\n",
    "# Load the test dataset\n",
    "test_set_root='./comp5623m-artificial-intelligence/test_set/test_set/'\n",
    "test_dataset = datasets.ImageFolder(test_set_root, transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# Create a list of image filenames for the test set\n",
    "image_filenames = [os.path.basename(path) for path, _ in test_dataset.imgs]\n",
    "\n",
    "\n",
    "# Load the saved model\n",
    "model=model_CNN\n",
    "model.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model))\n",
    "model.to(device)\n",
    "\n",
    "model.eval()\n",
    "# Make predictions on the test dataset\n",
    "all_predictions = []\n",
    "for inputs, _ in test_dataloader:\n",
    "    with torch.no_grad():\n",
    "        inputs=inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "print('predictions of all test images successfully! ')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle [8 marks]\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, ie., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "Please note you will get marks for higher performance.\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image in the test set and 1 row for the headers. [To submit please visit](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [8 marks]. The class leaderboard will not affect marking (brownie points!).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import csv\n",
    " # Save the image names and predicted classes to a CSV file\n",
    "with open('sc21xz.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Category'])\n",
    "    for i in range(len(test_dataset)):\n",
    "        writer.writerow([image_filenames[i], all_predictions[i]])\n",
    "print('CSV file successfully saved!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [20 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one task and then tunes or tweaks the model to make it perform a second similar task. You can perform finetuning in the following way:\n",
    "- Train an entire model from scratch (large dataset, more computation)\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation)\n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10``, [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here are only 10*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "# Define transforms for the training data and validation data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "batch_size=64\n",
    "# Download the dataset\n",
    "dataset_cifr10 = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "# Split the dataset into training and validation set\n",
    "train_dataloader_CIFAR10, val_dataloader_CIFAR10 = split_train_val_dataset(dataset_cifr10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import torchvision.models as models\n",
    "# Load pretrained AlexNet\n",
    "model_alexnet = models.alexnet(pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 Apply transfer learning initialise with pretrained model weights\n",
    "Use pretrained weights from AlexNet only (on the right of figure) to initialise your model.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/cc/Comparison_image_neural_networks.svg\" style=\"width:1000px;height:400px;\">\n",
    "<caption><center> <u>Figure</u>: Two models are given here: LeNet and AlexNet for image classification. However, you have to use **only AlexNet**.</center></caption>\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters\n",
    "import copy\n",
    "# No frozen layers\n",
    "\n",
    "for param in model_alexnet.parameters():\n",
    "    param.requires_grad = True\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "# Copy the model\n",
    "model_alexnet_c1=copy.deepcopy(model_alexnet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters\n",
    "\n",
    "# Frozen base convolution blocks\n",
    "for name, param in model_alexnet.features.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3 Compare above configurations and comment on comparative performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell\n",
    "def train_val_alex(epochs,model,loss_function, optimizer,train_dataloader, validation_dataloader):\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    train_acc_history=[]\n",
    "    val_acc_history=[]\n",
    "    best_val_acc=0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0\n",
    "        train_acc=0\n",
    "        val_loss=0\n",
    "        val_acc=0\n",
    "        # Training\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "            train_acc += acc.item()\n",
    "        train_loss /= len(train_dataloader)\n",
    "        train_acc /= len(train_dataloader)\n",
    "        train_loss_history.append(train_loss/len(train_dataloader))\n",
    "        train_acc_history.append(train_acc/len(train_dataloader))\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(validation_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                acc = topk_accuracy(outputs, labels, topk=(1,))[0]\n",
    "                val_acc += acc.item()\n",
    "        val_loss /= len(validation_dataloader)\n",
    "        val_acc /= len(validation_dataloader)\n",
    "        val_loss_history.append(val_loss/len(validation_dataloader))\n",
    "        val_acc_history.append(val_acc/len(validation_dataloader))\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        # Save the model if it has the best validation accuracy so far\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model_alex.pth')\n",
    "    return train_loss_history,val_loss_history,train_acc_history,val_acc_history\n",
    "\n",
    "\n",
    "\n",
    "print('start!')\n",
    "epochs = 10\n",
    "learning_rate=0.001\n",
    "\n",
    "# use model_c1:\n",
    "model_alex_c1 = model_alexnet_c1\n",
    "# output of alexnet is 1000, change it into 10\n",
    "model_alex_c1.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "# Define the loss function and optimizer\n",
    "loss_function_alex = nn.CrossEntropyLoss()\n",
    "optimizer_alex = torch.optim.Adam(model_alex_c1.classifier[6].parameters(), lr=learning_rate)\n",
    "# train the model\n",
    "model_alexnet_c1.to(device)\n",
    "train_loss_history_c1,val_loss_history_c1,train_acc_history_c1,val_acc_history_c1=train_val_alex(epochs, model_alex_c1, loss_function_alex, optimizer_alex,train_dataloader_CIFAR10, val_dataloader_CIFAR10 )\n",
    "\n",
    "# use model_c2:\n",
    "model_alex_c2=model_alexnet\n",
    "# output of alexnet is 1000, change it into 10\n",
    "model_alex_c2.classifier[6] = torch.nn.Linear(4096, 10)\n",
    "# Define the  optimizer\n",
    "optimizer_alex = torch.optim.SGD(model_alex_c2.parameters(), lr=learning_rate)\n",
    "# train the model\n",
    "model_alex_c2.to(device)\n",
    "train_loss_history_c2,val_loss_history_c2,train_acc_history_c2,val_acc_history_c2=train_val_alex(epochs, model_alex_c2, loss_function_alex, optimizer_alex, train_dataloader_CIFAR10, val_dataloader_CIFAR10)\n",
    "\n",
    "\n",
    "# Plot training and validation loss and accuracy\n",
    "# Plot loss and accuracy values over epochs\n",
    "plt.plot(train_loss_history_c1, label='Training Loss_c1')\n",
    "plt.plot(val_loss_history_c1, label='Validation Loss_c1')\n",
    "plt.plot(train_loss_history_c2, label='Training Loss_c2')\n",
    "plt.plot(val_loss_history_c2, label='Validation Loss_c2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_acc_history_c1, label='Training Accuracy_c1')\n",
    "plt.plot(val_acc_history_c1, label='Validation Accuracy_c1')\n",
    "plt.plot(train_acc_history_c2, label='Training Accuracy_c2')\n",
    "plt.plot(val_acc_history_c2, label='Validation Accuracy_c2')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5 Model comparisons\n",
    "We often need to compare our model with other state-of-the-art methods to understand how well it performs compared to existing architectures. Here you will thus compare your model design with AlexNet on the TinyImageNet30 dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.1 Finetune AlexNet on TinyImageNet30\n",
    "> Load AlexNet as you did above\n",
    "\n",
    "> Train AlexNet on TinyImageNet30 dataset until convergence. Make sure you use the same dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# load dataset\n",
    "transform_alex= transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "\n",
    "dataset = TinyImageNet30Dataset(root_dir=train_set_root,classes=classes, transform=transform_alex)\n",
    "dataloader = DataLoader(dataset,batch_size=batch_size)\n",
    "train_dataloader,val_dataloader=split_train_val_dataset(dataset)\n",
    "# Get a single batch of data\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "# load AlexNet\n",
    "model_alex = model_alexnet\n",
    "# change output into 30\n",
    "model_alex.classifier[-1] = torch.nn.Linear(4096, 30)\n",
    "\n",
    "# Define the parameters for training\n",
    "epochs = 30\n",
    "learning_rate=0.01\n",
    "#\n",
    "# AlexNet\n",
    "epochs = 50\n",
    "learning_rate=0.005\n",
    "loss_function_alex = nn.CrossEntropyLoss()\n",
    "model_alex = model_alex.to(device)\n",
    "optimizer_alex = torch.optim.SGD(model_alex.parameters(), lr=learning_rate,momentum=0.9)\n",
    "train_loss_history_alex,val_loss_history_alex,train_acc_history_alex,val_acc_history_alex=train_val_alex(epochs, model_alex, loss_function_alex, optimizer_alex, train_dataloader, val_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2 Compare results on validation set of TinyImageNet30\n",
    "> Loss graph, top1 accuracy, confusion matrix and execution time for your model (say, mymodel and AlexNet)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "import time\n",
    "\n",
    "# loss graph\n",
    "plt.plot(val_loss_history_alex, label='Validation Loss_alex')\n",
    "plt.plot(val_loss_history_mymodel, label='Validation Loss_mymodel')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# top1 accuracy\n",
    "plt.plot(val_acc_history_alex, label='Validation Accuracy_alex')\n",
    "plt.plot(val_acc_history_mymodel, label='Validation Accuracy_mymodel')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# load CNN model\n",
    "model_cnn=model_CNN\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model_cnn))\n",
    "\n",
    "# load model_alex\n",
    "model_alex=model_alexnet\n",
    "model_alex.load_state_dict(torch.load('best_model_alex.pth'))\n",
    "print(type(model_alex))\n",
    "\n",
    "# confusion matrix of CNN\n",
    "model_cnn.eval()\n",
    "start_time_cnn = time.time()\n",
    "# confusion matrix\n",
    "preds_cnn = []\n",
    "labels_cnn = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model_cnn(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_cnn.extend(preds.tolist())\n",
    "        labels_cnn.extend(labels.tolist())\n",
    "print('get predictions for the CNN set successfully!')\n",
    "end_time_cnn = time.time()\n",
    "\n",
    "\n",
    "# confusion matrix of AlexNet\n",
    "start_time_alex = time.time()\n",
    "model_alex.eval()\n",
    "# confusion matrix\n",
    "preds_alex = []\n",
    "labels_alex = []\n",
    "# Evaluate the model on the validation set\n",
    "for inputs, labels in val_dataloader:\n",
    "    inputs=inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model_alex(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        # Append predictions and labels to lists\n",
    "        preds_alex.extend(preds.tolist())\n",
    "        labels_alex.extend(labels.tolist())\n",
    "print('get predictions for the Alexnet set successfully!')\n",
    "end_time_alex = time.time()\n",
    "\n",
    "# Compute the confusion matrices\n",
    "cm_alex = confusion_matrix(labels_alex, preds_alex)\n",
    "# Normalize the confusion matrices\n",
    "cm_norm_alex = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "# Print the confusion matrices\n",
    "print(\"AlexNet Confusion Matrix:\")\n",
    "print(cm_norm_alex)\n",
    "\n",
    "# Compute the confusion matrices\n",
    "cm_cnn = confusion_matrix(labels_cnn, preds_cnn)\n",
    "# Normalize the confusion matrices\n",
    "cm_norm_cnn = val_cm.astype('float') / val_cm.sum(axis=1)[:, np.newaxis]\n",
    "# Print the confusion matrices\n",
    "print(\"My Model Confusion Matrix:\")\n",
    "print(cm_norm_cnn)\n",
    "\n",
    "\n",
    "# compute execution time\n",
    "execution_time = end_time_alex - start_time_alex\n",
    "print('execution time of AlexNet is: ', execution_time)\n",
    "execution_time = end_time_cnn - start_time_cnn\n",
    "print('execution time of CNN is: ', execution_time)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6 Interpretation of results (16 marks)\n",
    "\n",
    "> Please use TinyImageNet30 dataset for all results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "#### 6.1-6.2 Implement grad-CAM and visualise results (10 marks)\n",
    "\n",
    "- Use an existing library to initiate grad-CAM\n",
    "\n",
    "        - To install: !pip install torchcam\n",
    "        - Call SmoothGradCAMpp: from torchcam.methods import SmoothGradCAMpp\n",
    "        - Apply to your model\n",
    "\n",
    "You can see the details here: https://github.com/frgfm/torch-cam\n",
    "\n",
    "- Apply grad-CAM to your model on at least four correctly classified images\n",
    "- Apply grad-CAM on retrained AlexNet on at least four incorrectly classified images\n",
    "\n",
    ">It is recommended to first read the relevant paper [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391), and refer to relevant course material.\n",
    "\n",
    "\n",
    "**HINT for displaying images with grad-CAM:**\n",
    "\n",
    "Display ```heatmap``` as a coloured heatmap superimposed onto the original image. We recommend the following steps to get a clear meaningful display.\n",
    "\n",
    "From torchcam.utils import overlay_mask. But remember to resize your image, normalise it and put a 1 for the batch dimension (e.g, [1, 3, 224, 224])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchcam.utils import overlay_mask\n",
    "\n",
    "image_paths=[]\n",
    "labels=[]\n",
    "root_dir='./comp5623m-artificial-intelligence/train_set/train_set/train_set'\n",
    "# load images_cnn_correct\n",
    "# Iterate over all classes\n",
    "for i, c in enumerate(classes):\n",
    "    class_dir = os.path.join(root_dir, c)\n",
    "    # print(i,c)\n",
    "    if os.path.isdir(class_dir):\n",
    "        # Add all images in this class directory to the dataset\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            image_paths.append(image_path)\n",
    "            labels.append(i)\n",
    "print(len(image_paths))\n",
    "\n",
    "def get_gradcam(model,model_name, image_paths, transform):\n",
    "\n",
    "    pred=0\n",
    "    model.eval()\n",
    "    # Loop over the images\n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        # Load the image and preprocess it\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        preprocess = transform\n",
    "        image_tensor= preprocess(image)\n",
    "        input_tensor = image_tensor.unsqueeze(0)\n",
    "        # Get the predicted class index\n",
    "        output = model(input_tensor)\n",
    "        # Retrieve the CAM by passing the class index and the model output\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        if predicted.item()==labels[i] and model_name == 'cnn':\n",
    "            print('predicted class is: ', predicted.item())\n",
    "            print('true class is: ', labels[i])\n",
    "            print('correctly classified')\n",
    "            pred+=1\n",
    "            cam_extractor = SmoothGradCAMpp(model, target_layer='layer3.3')\n",
    "            # Preprocess your data and feed it to the model\n",
    "            out = model(input_tensor)\n",
    "            # Retrieve the CAM by passing the class index and the model output\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "            # Resize the CAM and overlay it\n",
    "            result = overlay_mask(to_pil_image(image_tensor), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "            # Display it\n",
    "            plt.imshow(result);\n",
    "            plt.axis('off');\n",
    "            plt.tight_layout();\n",
    "            plt.show()\n",
    "            # Show the ordinary image using the default viewer on your system\n",
    "            img = plt.imread(image_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        if predicted.item()!=labels[i] and model_name == 'alexnet':\n",
    "            print('predicted class is: ', predicted.item())\n",
    "            print('true class is: ', labels[i])\n",
    "            print('incorrectly classified!')\n",
    "            pred+=1\n",
    "\n",
    "            cam_extractor = SmoothGradCAMpp(model, target_layer='features.12')\n",
    "            # Preprocess your data and feed it to the model\n",
    "            out = model(input_tensor)\n",
    "            # Retrieve the CAM by passing the class index and the model output\n",
    "            activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)\n",
    "\n",
    "            # Resize the CAM and overlay it\n",
    "            result = overlay_mask(to_pil_image(image_tensor), to_pil_image(activation_map[0].squeeze(0), mode='F'), alpha=0.5)\n",
    "            # Display it\n",
    "            plt.imshow(result);\n",
    "            plt.axis('off');\n",
    "            plt.tight_layout();\n",
    "            plt.show()\n",
    "            # Show the ordinary image using the default viewer on your system\n",
    "            img = plt.imread(image_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        if pred==4:\n",
    "                break\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Load the saved model\n",
    "learning_rate=0.01\n",
    "model_cnn=CNNWithDropout(num_classes=30)\n",
    "model_cnn.load_state_dict(torch.load('best_model_cnn.pth'))\n",
    "print(type(model_cnn))\n",
    "# model_cnn.to(device)\n",
    "transform_cnn = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "get_gradcam(model_cnn, 'cnn',image_paths, transform_cnn)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#AlexNet\n",
    "# # load model_alex\n",
    "model_alex = models.alexnet(pretrained=True)\n",
    "for param in model_alexnet.parameters():\n",
    "    param.requires_grad = True\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "# Frozen base convolution blocks\n",
    "for name, param in model_alexnet.features.named_parameters():\n",
    "    param.requires_grad = False\n",
    "# Print trainable parameters\n",
    "for name, param in model_alexnet.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "model_alex.classifier[6] = torch.nn.Linear(4096, 30)\n",
    "model_alex.load_state_dict(torch.load('best_model_alex.pth'))\n",
    "print(type(model_alex))\n",
    "transform_alex= transforms.Compose(\n",
    "    [transforms.Resize((224,224)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5,0.5)])\n",
    "\n",
    "get_gradcam(model_alex, 'alexnet',image_paths, transform_alex)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 6.3 Your comments on (6 marks):\n",
    "> a) Why model predictions were correct or incorrect? You can support your case from 6.2\n",
    "\n",
    "> b) What can you do to improve your results further?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "a)\n",
    "The heat map generated by SmoothGradCAMpp shows which areas of the model play a decisive role in the prediction results during the prediction process.\n",
    "\n",
    "In 6.2, we output the comparison between the heat map and the original map to help analyse the reason why the model prediction is good or not.\n",
    "\n",
    "When using the CNN model to classify images, although the correctly classified heat map generally matches our intuition and corresponds to the features in the original image (e.g. it can be basically judged where the objects that need to be recognized in the picture are distributed, such as the Figure 2 and Figure 3), but the specific characteristics cannot be fully clearly identified (such as the limbs and heads of animals in class 0 are not recognized, such as Figure 3).And for Figure 1 and Figure 4, even the main object positions are not been recognized. Therefore, we found that although the general features were captured, it was not very effective in helping the model make correct predictions. This may be the reason why the CNN model can accurately classify these images but the accuracy rate is still relatively low.\n",
    "\n",
    "\n",
    "When using the AlexNet model to classify images, its incorrectly classified heat map is completely inconsistent with our intuition. Even the distribution area of key objects is not located. In fact, the heat map basically focuses on the four corners of the picture. This may be the reason why the accuracy of the AlexNet model is relatively high, but these pictures are still unable to be accurately classified.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "b)\n",
    "To improve results, the best way is to increase the amount of data in the training set. So that the model can better identify key features and increase the importance of these features. For example, for the CNN model, it is necessary to strengthen the identification of some key features. And it is same for the ALexnet model. Increasing the amount of data can help it distinguish key features to improve training accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission. Use our teams channel to seek any help!**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
